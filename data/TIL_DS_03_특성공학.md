# Feature Engineering(특성 공학)
- 인공지능 알고리즘에 적용, 빅데이터 분석 등을 위해 데이터에 대한 도메인 지식을 활용하여 특성(Feature)를 만들어내는 과정
- 추출된 특성은 기계 학습 알고리즘의 입력으로 사용되어 학습 모델의 성능을 향상시키는 데 도움이 된다.
- 데이터 전처리 과정에서 가장 중요한 부분 중 하나이다. 
- 데이터를 분석하고, 특성을 추출하고, 특성 간의 상호작용을 고려하여 새로운 특성을 생성한다. 
- 예를 들면, 이미지 인식 분야에서는 이미지의 색상, 밝기, 질감 등의 특성을 추출하고, 이러한 특성을 조합하여 이미지 내 객체를 인식할 수 있는 모델을 만든다.

## 특성공간 차원축소의 필요성
- 모델의 해석력 향상.
- 모델 훈련시간의 단축.
- 차원의 저주 방지.
- 과적합(overfitting)에 의한 일반화 오차를 줄여 성능 향상.

# 특성공학의 방법론
## 특성 선택(feature selection)
- 주어진 특성 변수들 가운데 가장 좋은 특성변수의 조합만 선택함.
- 불필요한 특성 변수를 제거함.
- Filtering, Wrapper, Embedded 방식

![image](https://user-images.githubusercontent.com/82266289/235358749-00c6629b-ea65-4f4f-964f-28b5defb3e43.png)

### Filter 방식: 각 특성변수를 독립적인 평가함수로 평가함.
- 각 특성변수 $X_i$와 목표변수(Y)와의 연관성을 측정한 뒤, 목표변수를 잘 설명할 수 있는 특성변수만을 선택하는 방식.
- $X_i$와 Y의 1:1 관계로만 연관성을 판단.
- 연관성 파악을 위해 t-test, chi-square test, information gain

### Wrapper 방식: 학습 알고리즘을 이용.
- 다양한 특성변수의 조합에 대해 목표변수를 예측하기 위한 알고리즘을 훈련하고, corss-validation 등의 방법으로 훈련된 모델의 예측력을 평가함. 그 결과를 비교하여 최적화된 특성변수의 조합을 찾는 방법.
- 특성변수의 조합이 바뀔때마다 모델을 학습함.
- 특성변수에 중복된 정보가 많은 경우 이를 효과적으로 제거함.
- 대표적인 방법으로는 순차탐색법인 forward selection, backward selection, stepwise selection등이 있음.

### Filter와 Wrapper의 장단점 비교
|   | 장점 | 단점 |
|---|------|------|
|Filter|- 계산 비용이 적고 속도가 빠름.|- 특성변수간의 상호작용을 고려하지 않음.|
|Wrapper|- 특성변수 간의 상호작용을 고려함.<br> - 주어진 학습 알고리즘에 대해 항상 최적의 특성변수 조합을 찾음.| - 모델을 학습해야 하므로, 계산비용이 크고 속도가 느림.<br> - 과적합(overfitting)의 가능성 있음.|

### Embedded 방식 : 학습 알고리즘 자체에 feature selection을 포함하는 경우
- Wrapper 방식은 모든 특성변수 조합에 대한 학습을 마친 결과를 비교하는데 비해, Embedded 방식은 학습 과정에서 최적화된 변수를 선택한다는 점에서 차이가 있음.
- 대표적인 방법으로는 특성변수에 규제를 가하는 방식인 Ridge, Lasso, Elastic net 등이 있음.

## 특성 추출(feature extraction)

