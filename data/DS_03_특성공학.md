# Feature Engineering(특성 공학)
- 인공지능 알고리즘에 적용, 빅데이터 분석 등을 위해 데이터에 대한 도메인 지식을 활용하여 특성(Feature)를 만들어내는 과정
- 추출된 특성은 기계 학습 알고리즘의 입력으로 사용되어 학습 모델의 성능을 향상시키는 데 도움이 된다.
- 데이터 전처리 과정에서 가장 중요한 부분 중 하나이다. 
- 데이터를 분석하고, 특성을 추출하고, 특성 간의 상호작용을 고려하여 새로운 특성을 생성한다. 
- 예를 들면, 이미지 인식 분야에서는 이미지의 색상, 밝기, 질감 등의 특성을 추출하고, 이러한 특성을 조합하여 이미지 내 객체를 인식할 수 있는 모델을 만든다.

## 특성공간 차원축소의 필요성
- 모델의 해석력 향상.
- 모델 훈련시간의 단축.
- 차원의 저주 방지.
- 과적합(overfitting)에 의한 일반화 오차를 줄여 성능 향상.

# 특성공학의 방법론
## 특성 선택(feature selection)
- 주어진 특성 변수들 가운데 가장 좋은 특성변수의 조합만 선택함.
- 불필요한 특성 변수를 제거함.
- Filtering, Wrapper, Embedded 방식

![image](https://user-images.githubusercontent.com/82266289/235358749-00c6629b-ea65-4f4f-964f-28b5defb3e43.png)

### Filter 방식: 각 특성변수를 독립적인 평가함수로 평가함.
- 각 특성변수 $X_i$와 목표변수(Y)와의 연관성을 측정한 뒤, 목표변수를 잘 설명할 수 있는 특성변수만을 선택하는 방식.
- $X_i$와 Y의 1:1 관계로만 연관성을 판단.
- 연관성 파악을 위해 t-test, chi-square test, information gain

### Wrapper 방식: 학습 알고리즘을 이용.
- 다양한 특성변수의 조합에 대해 목표변수를 예측하기 위한 알고리즘을 훈련하고, corss-validation 등의 방법으로 훈련된 모델의 예측력을 평가함. 그 결과를 비교하여 최적화된 특성변수의 조합을 찾는 방법.
- 특성변수의 조합이 바뀔때마다 모델을 학습함.
- 특성변수에 중복된 정보가 많은 경우 이를 효과적으로 제거함.
- 대표적인 방법으로는 순차탐색법인 forward selection, backward selection, stepwise selection등이 있음.

### Filter와 Wrapper의 장단점 비교
|   | 장점 | 단점 |
|---|------|------|
|Filter|- 계산 비용이 적고 속도가 빠름.|- 특성변수간의 상호작용을 고려하지 않음.|
|Wrapper|- 특성변수 간의 상호작용을 고려함.<br> - 주어진 학습 알고리즘에 대해 항상 최적의 특성변수 조합을 찾음.| - 모델을 학습해야 하므로, 계산비용이 크고 속도가 느림.<br> - 과적합(overfitting)의 가능성 있음.|

### Embedded 방식 : 학습 알고리즘 자체에 feature selection을 포함하는 경우
- Wrapper 방식은 모든 특성변수 조합에 대한 학습을 마친 결과를 비교하는데 비해, Embedded 방식은 학습 과정에서 최적화된 변수를 선택한다는 점에서 차이가 있음.
- 대표적인 방법으로는 특성변수에 규제를 가하는 방식인 Ridge, Lasso, Elastic net 등이 있음.

## 특성 추출(feature extraction)
- 가지고 있는 특성을 결합하여 더 유용한 특성을 생성.

### 주요 특성 추출법
- PCA(Principal component analysis)
- SVD(Singular Value Decomposition)
- LDA(Linear discriminant analysis)
- NMF(Non-negative matrix factorization)

# 주성분 분석(PCA)
- 서로 연관되어 있는 변수들($x_1$,...,$x_k$)이 관찰되었을 때, 이 변수들이 전체적으로 가지고 있는 정보들을 최대한 확보하는 적은 수의 새로운 변수(주성분, PC)를 생성하는 방법

## 주성분 분석의 목적
- 자료의 변동이 큰 축을 탐색함.
- 변수들에 담긴 정보의 손실을 최소화하면서 차원을 축소함.
- 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이하게 함.

## 주성분 분석 아이디어
- k개의 특성변수 $x_1$,...,$x_k$의 주성분이 $y_1$,...,$y_k$라면 이들은 $x_1$,...,$x_k$의 선형결합식으로 아래와 같이 표현됨.
- $y_1 = l_{11}x_1 + l_{21}x_2 + \dots + l_{k1}x_k$
- $y_2 = l_{12}x_1 + l_{22}x_2 + \dots + l_{k2}x_k$
<br><center>$\ldots$</center>
- $y_k = l_{11}x_1 + l_{21}x_2 + \dots + l_{k1}x_k$

![image](https://user-images.githubusercontent.com/82266289/235468573-5abdba9c-d831-499a-bf95-964182c7d347.png)
1. $V[y_1]$를 최대하로 하는 길이가 1인 벡터 $l_1 = (l_{11},l_{21},...,l_{k1})$로 첫번째 주성ㅂ운 $y_1$을 결정.
2. $Cov[y_2,y_1] = 0$을 만족하며 $V[y_2]$를 최대로 하는 길이가 1인 벡터 $l_2 = (l_{12},l_{22},...,l_{k2})로 두번째 주성분 y_2을 결정.
3. $Cov[y_j,y_m] = 0 (m < j>)$을 만족하며 $V[y_j]$를 최대로 하는 길이가 1인 벡터 $l_j = (l_{1j},l_{2j},...,l_{kj})로 $j$번째 주성분 $y_j$을 결정. ($j = 3,...,k$에 대하여 이 과정을 반복)

## 주성분 분석에 관한 기하학적 의미
### 주성분 축은 원래 변수들의 좌표측이 직교 회전 변환된 것으로 해석할 수 있음
- 첫번째 주성분 축은 데이터의 변동이 가장 커지는 축임.
- 두번째 주성분 축은 첫번째 주성분 축과 직교하여 첫번째 주성분 축 다음으로 데이터의 변동이 큰 축을 나타냄.
- 각 관찰치 별 주성분 점수는 대응하는 원 자료 값들의 주성분 좌표축에서의 좌표 값에 해당함.
- 자료들의 공분산 행렬이 대각행렬이 되도록 회전한 것으로 해석할 수 있음.

### 주성분 축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석할 수 있음.
![image](https://user-images.githubusercontent.com/82266289/235469692-adbc2b7a-a6c1-4a16-a515-da49d9a05f8a.png)

# 특성값분해(SVD)
### 임의의 $n*d$ 행렬 $A$는 $𝐴=𝑈Σ𝑉^𝑇$로 분해 가능함.
- $U$와 $V$는 직교행렬: $U^TU = l_{n*n}$, $VV^T = l_{d*d}$
- $U$의 각 열을 $A$의 왼쪽 특성벡터, $V$의 각 열을 $A$의 오른쪽 특성벡터라고 함.
- Σ는 $n*d$의 대각행렬: 대각원소를 $A$의 특성값이라고 함.

![image](https://user-images.githubusercontent.com/82266289/235470769-57e01f39-29c3-4642-8409-067d6e999080.png)

### 특이값 분해와 차원축소
- $U$의 각 열을 $u_i,i = 1,...,n$
- $V^T$의 각 행을 $v^T_i$, $i = 1,...d$
- Σ의 0이 아닌 대각원소를 $\lambda_i$, $i = 1,...r(\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_r )$이라고 할 때,
<br><br>
$\textbf{A}=\textbf{U}\Sigma\textbf{V}^T=\lambda_1\textbf{u}_1\textbf{v}_1^T+\lambda_2\textbf{u}_2\textbf{v}_2^T+\cdots+\lambda_m\textbf{u}_m\textbf{v}_m^T+\cdots+\lambda_r\textbf{u}_r\textbf{v}_r^T$

<br><br>
정보가 만은 순서대로 m개만 이용하여 근사하는 경우 m계수 근사라고 함.

### 주성분 분석(PCA)와 특성값분해의 관계
- A의 오른쪽 특성벡터는 A의 공분산행렬의 고유벡터와 동일함
- 자료 행렬의 대한 특성값 분해로 주성분을 도출가능.