# Apache Spark 개요

- Apache Spark는 대용량 데이터 처리를 위해 설계된 오픈 소스 분산 컴퓨팅 시스템이다. 
- Spark는 메모리 기반의 데이터 처리를 통해 기존의 디스크 기반의 하둡(MapReduce) 시스템보다 빠른 처리 속도를 제공한다. 
- 이는 대규모 데이터셋의 분석 및 처리에 탁월한 성능을 발휘한다.

### Spark의 핵심 구성 요소

1. **RDD(Resilient Distributed Dataset)**: 불변성을 가진 분산 데이터 컬렉션으로, Spark의 기본 데이터 구조이다. RDD를 통해 데이터의 병렬 처리가 가능하다.

2. **스파크 코어(Spark Core)**: 스파크의 기본 기능을 담당하며, 태스크 스케줄링, 메모리 관리, 오류 복구 등의 기능을 제공한다.

3. **스파크 SQL(Spark SQL)**: SQL 쿼리를 사용하여 데이터를 처리할 수 있게 해주는 모듈이다.

4. **스파크 스트리밍(Spark Streaming)**: 실시간 데이터 스트리밍 처리를 지원한다.

5. **MLlib(Machine Learning Library)**: 머신 러닝 알고리즘과 유틸리티를 제공한다.

6. **GraphX**: 그래프 기반의 데이터 처리를 위한 라이브러리이다.

### Apache Spark의 장점 및 단점

|             | Apache Spark                                                        |
|-------------|---------------------------------------------------------------------|
| 장점        | - 빠른 처리 속도와 확장성을 제공하여 대규모 데이터 처리에 적합하다.   |
|             | - 메모리 기반의 데이터 처리로 디스크 I/O에 비해 더 빠른 성능을 보여준다.|
|             | - 다양한 언어로 개발할 수 있는 API를 제공하여 유연한 개발이 가능하다. |
|             | - 다목적 클러스터 컴퓨팅 시스템으로 다양한 작업 유형을 지원한다.       |
|             | - 다양한 기능과 라이브러리를 제공하여 다양한 분야에서 활용이 가능하다. |
| 단점        | - 메모리 사용량이 많아 대규모 클러스터가 필요할 수 있다.              |
|             | - 머신러닝 구현 시 학습 곡선이 다소 가파르며, 초기 설정과 관련된 어려움이 있다. |
|             | - 데이터 분산 처리에 따른 복잡성이 존재할 수 있다.                    |

### Spark 사용 사례

1. **데이터 분석 및 처리**: 대규모 데이터셋의 분석 및 처리를 위해 사용된다.

2. **실시간 데이터 처리**: 스트리밍 데이터를 실시간으로 처리하는데 사용할 수 있다.

3. **머신 러닝**: MLlib을 사용하여 데이터 분석 및 예측 모델을 구축한다.

4. **그래프 처리**: GraphX를 통해 복잡한 그래프 알고리즘과 분석을 수행한다.

